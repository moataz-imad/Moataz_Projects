{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# from sql_metadata import Parser\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=\"project_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Table `bigquery -> pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"SELECT * FROM reports.table limit 5\"\n",
    "dataframe = client.query(query).to_dataframe()\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_jobs(tables):\n",
    "    # combine tables into a sql format \n",
    "    tables_sql = \", \".join(f\"'{table}'\" for table in tables)\n",
    "    # query below gets the most up to date query for each table \n",
    "    query = f\"\"\"\n",
    "    with jobs as (\n",
    "    SELECT\n",
    "        -- job_id, \n",
    "        concat(destination_table.dataset_id,'.',destination_table.table_id) table_name,\n",
    "        row_number() over (partition by concat(destination_table.dataset_id,'.',destination_table.table_id) order by creation_time desc) rank,\n",
    "        user_email,\n",
    "        creation_time,\n",
    "        query\n",
    "    FROM `region-us.INFORMATION_SCHEMA.JOBS_BY_PROJECT`\n",
    "    WHERE \n",
    "        concat(destination_table.dataset_id,'.',destination_table.table_id) IN ({tables_sql})\n",
    "        AND state = 'DONE'\n",
    "    )\n",
    "    select * from jobs\n",
    "    where rank = 1\n",
    "    \"\"\"\n",
    "    query_job = client.query(query)\n",
    "    try:\n",
    "        df = query_job.result().to_dataframe()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        df = pd.DataFrame()\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets=['reports','manual']\n",
    "root='queries'\n",
    "tables=[]\n",
    "# create folders if they do not exist\n",
    "for subset in subsets:\n",
    "    print('\\n'+'-'*50+f'\\n{subset}:')\n",
    "    print(f'views:')\n",
    "    v_count=0\n",
    "    t_count=0\n",
    "    folder_path=os.path.join(root,subset)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    element_list=list(client.list_tables(f'project_name.{subset}'))\n",
    "    for element in element_list:\n",
    "        element_ref = element.reference\n",
    "        element_obj = client.get_table(element_ref)\n",
    "        # help(element_obj)\n",
    "        #----------------------------------------------- IF IT IS A VIEW WITH A QUERY:\n",
    "        if element_obj.view_query is not None:\n",
    "            v_count+=1\n",
    "            view_query=element_obj.view_query.replace('\\r\\n','\\n')\n",
    "            # print(repr(view_query[:500])) # one issue that was faced is i couldn't tell if it was a \\n\\n or \\r\\n, repr shows which\n",
    "            view_id=element_obj.table_id\n",
    "            full_path=os.path.join(folder_path, view_id +'.sql')\n",
    "            with open(full_path,'w') as f:\n",
    "                f.write(view_query)\n",
    "                pass\n",
    "            print(v_count,element_obj.table_id,end=', ')\n",
    "            if v_count%10==0:\n",
    "                print('')\n",
    "            # break\n",
    "        else:\n",
    "            # list tables\n",
    "            tables.append(element.dataset_id + '.' + element.table_id)\n",
    "    #----------------------------------------------- IF IT IS A TABLE BASED ON A SCHEDULED QUERY: \n",
    "    t_df = search_jobs(tables)\n",
    "    \n",
    "    print(f'\\ntables:')\n",
    "    for i,r in t_df.iterrows():\n",
    "        if r.query is not None:\n",
    "            t_count+=1\n",
    "            table_query=r.query.replace('\\r\\n','\\n')\n",
    "            parent,table_id = r.table_name.split(\".\")\n",
    "            table_id = f't_{table_id}'\n",
    "            full_path=os.path.join(root,parent, table_id +'.sql')\n",
    "            with open(full_path,'w') as f:\n",
    "                f.write(table_query)\n",
    "                pass\n",
    "            print(t_count,table_id,end=', ')\n",
    "            if t_count%10==0:\n",
    "                print('')\n",
    "                # break       \n",
    "    print(f'\\n{v_count}/{len(element_list)} elements are views with query')\n",
    "    print(f'{t_count}/{len(element_list)} elements are tables with query')\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sql_search\"></a>\n",
    "### SQL Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "# from sql_metadata import Parser\n",
    "\n",
    "pd.set_option('display.max_colwidth', 120)  # Show full column content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_search(word_1, word_2, method='both',approximate=False,path=\"queries/*/*.sql\", with_comments=False):\n",
    "    # Validate method\n",
    "    valid_methods = {'both', 'any', 'word_1', 'word_2','word_1_only'}\n",
    "    if method not in valid_methods:\n",
    "        raise ValueError(f\"Invalid method. Choose from {valid_methods}.\")\n",
    "\n",
    "    # Find all .sql files in the queries/*/ directory\n",
    "    sql_files = glob.glob(path,recursive=True) # '../../../**/*.sql' for all the source control\n",
    "    print(f'searching {len(sql_files)} files')\n",
    "    matching_files = []\n",
    "\n",
    "    for sql_file in sql_files:\n",
    "        with open(sql_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            lines = [f'File Name: {sql_file}'] + lines\n",
    "        matches_infile = []\n",
    "        \n",
    "        word_1_found = False\n",
    "        word_2_found = False\n",
    "\n",
    "        for line_number, line in enumerate(lines, start=1):\n",
    "            normalized_line = line.strip().lower()\n",
    "            \n",
    "            cleaned_line = re.sub('--.*','',normalized_line) # with no comments\n",
    "            m={}\n",
    "            \n",
    "            # Check for word_1 using regex\n",
    "            if approximate:\n",
    "                pattern_1=rf'{re.escape(word_1.lower())}'\n",
    "                pattern_2=rf'{re.escape(word_2.lower())}'\n",
    "            else:\n",
    "                pattern_1=rf'\\b{re.escape(word_1.lower())}\\b'\n",
    "                pattern_2=rf'\\b{re.escape(word_2.lower())}\\b'\n",
    "                \n",
    "            if re.search(pattern_1, cleaned_line):\n",
    "                word_1_found = True\n",
    "                # matches.append(f\"Line {line_number}: {line.strip()} (word_1 match)\")\n",
    "                m['file']=sql_file\n",
    "                m['line_number']=line_number\n",
    "                m['line']=line.strip()\n",
    "                m['match']= word_1\n",
    "                matches_infile.append(m)\n",
    "\n",
    "            # Check for word_2 using regex\n",
    "            if re.search(pattern_2, cleaned_line):\n",
    "                word_2_found = True\n",
    "                # matches.append(f\"Line {line_number}: {line.strip()} (word_2 match)\")\n",
    "                m['file']=sql_file\n",
    "                m['line_number']=line_number\n",
    "                m['line']=line.strip()\n",
    "                m['match']= word_2\n",
    "                matches_infile.append(m)\n",
    "\n",
    "        # Determine if the file meets the criteria\n",
    "        if method == 'both' and word_1_found and word_2_found:\n",
    "            matching_files += matches_infile\n",
    "            \n",
    "        elif method == 'any' and (word_1_found or word_2_found):\n",
    "            matching_files += matches_infile\n",
    "        \n",
    "        elif method == 'word_1_only' and (word_1_found and not word_2_found):\n",
    "            matching_files += matches_infile\n",
    "            \n",
    "        elif method == 'word_1' and word_1_found:\n",
    "            matching_files += matches_infile\n",
    "            \n",
    "        elif method == 'word_2' and word_2_found:\n",
    "            matching_files += matches_infile\n",
    "    \n",
    "    results = pd.DataFrame(matching_files, columns=['file','line_number','line','match'])\n",
    "    \n",
    "    if len(matching_files) == 0:\n",
    "        print(f'no match!\\nfor {method}\\n word_1 = {word_1} and word_2 = {word_2}') \n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching 209 files\n",
      "9 matching files\n",
      "queries\\reports\\temp.sql\n",
      "queries\\reports\\v_autopay_profile.sql\n",
      "queries\\reports\\v_autopay_to_reimbursement.sql\n",
      "queries\\reports\\v_booked_deals_platform.sql\n",
      "queries\\reports\\v_company_onboarding.sql\n",
      "queries\\reports\\v_ee_roster.sql\n",
      "queries\\reports\\v_enrollment_app_qa.sql\n",
      "queries\\reports\\v_env_rate_validation.sql\n",
      "queries\\reports\\v_ye_reporting_contacts.sql\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# Search Configuration\n",
    "# ----------------------------------\n",
    "\n",
    "word_1 = \"isMigrated\"\n",
    "word_2 = \"20245\"\n",
    "method = \"any\"            # Options: 'both', 'any', 'word_1', 'word_2', 'word_1_only'\n",
    "path = 'queries/**/*.sql' # Use glob patterns to control search scope\n",
    "appx = True               # Set to False for strict word-boundary matches\n",
    "\n",
    "# Run search\n",
    "results = sql_search(word_1, word_2, method, appx, path)\n",
    "\n",
    "# ----------------------------------\n",
    "# Ignore List (optional filtering)\n",
    "# ----------------------------------\n",
    "\n",
    "ignorelist = [\n",
    "    r'queries\\manual\\v_report1.sql',\n",
    "    r'queries\\manual\\v_report2.sql',\n",
    "    r'queries\\manual\\v_report3.sql',\n",
    "]\n",
    "\n",
    "# Example: apply filters to refine search results\n",
    "# results = results.query('file in @ignorelist')                          # Include only ignored files\n",
    "# results = results[results.line.str.contains('\\(')]                     # Lines containing parentheses\n",
    "# results = results[~results.line.str.contains('SUBMITTED_TO_CARRIER')]  # Exclude lines with specific text\n",
    "\n",
    "# ----------------------------------\n",
    "# Output & Display\n",
    "# ----------------------------------\n",
    "\n",
    "matching_files = results['file'].unique()\n",
    "print(len(matching_files), 'matching files')\n",
    "for i in matching_files:\n",
    "    print(i)\n",
    "\n",
    "# Export results\n",
    "results.to_csv('matches.csv', index=False)\n",
    "\n",
    "# Optional: Highlight \"File Name\" headers in styled DataFrame\n",
    "def highlight_filename(s):\n",
    "    return ['color:#aaaa00;font-weight:bold' if 'File Name' in str(v) else '' for v in s]\n",
    "\n",
    "# Apply styling in notebook (if needed)\n",
    "# styled_df = results.style.apply(highlight_filename, subset=['line'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
